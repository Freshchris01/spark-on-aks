#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

FROM openjdk:8-jdk-slim

RUN set -ex && \
    apt-get update && \
    ln -s /lib /lib64 && \
    apt install -y bash tini libc6 libpam-modules libnss3 wget && \
    mkdir -p /opt/spark && \
    mkdir -p /opt/spark/work-dir && \
    touch /opt/spark/RELEASE && \
    rm /bin/sh && \
    ln -sv /bin/bash /bin/sh && \
    echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su && \
    chgrp root /etc/passwd && chmod ug+rw /etc/passwd && \
    rm -rf /var/cache/apt/*



# Setup path
RUN mkdir -p /opt/spark
RUN mkdir -p /opt/hadoop



# Download Spark and Hadoop Binaries
RUN wget https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-without-hadoop-scala-2.12.tgz
RUN wget https://downloads.apache.org/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz

# Extract tar all to path. Exclude certain files to reduce docker build time and container size
RUN tar -xzvf spark-2.4.5-bin-without-hadoop-scala-2.12.tgz -C /opt/spark/ --exclude='*.html' --exclude='*.txt' --strip-components 1
RUN tar -xzvf hadoop-3.2.1.tar.gz -C /opt/hadoop/ --exclude='*.html' --exclude='*.txt' --exclude='*.png' --exclude='*.jpg' --exclude='*.gif' --strip-components 1

# Clean up tar ball
RUN rm spark-2.4.5-bin-without-hadoop-scala-2.12.tgz
RUN rm hadoop-3.2.1.tar.gz



# Setup ENV
ENV SPARK_HOME /opt/spark
ENV PATH="$SPARK_HOME/bin:$PATH"

ENV HADOOP_HOME=/opt/hadoop
ENV HADOOP_INSTALL=$HADOOP_HOME
ENV HADOOP_MAPRED_HOME=$HADOOP_HOME
ENV HADOOP_COMMON_HOME=$HADOOP_HOME
ENV HADOOP_HDFS_HOME=$HADOOP_HOME
ENV YARN_HOME=$HADOOP_HOME
ENV HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
ENV LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
ENV PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
ENV HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
#configure hadoop jars
RUN echo "export SPARK_DIST_CLASSPATH=$(hadoop classpath):$HADOOP_HOME/share/hadoop/tools/lib/*" >> ~/.bashrc 
RUN cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
RUN echo "export HADOOP_HOME=/opt/hadoop" >> $SPARK_HOME/conf/spark-env.sh
RUN echo "export PATH=$SPARK_HOME/bin:$HADOOP_HOME/bin:$PATH" >> $SPARK_HOME/conf/spark-env.sh
RUN echo "export SPARK_DIST_CLASSPATH=$(hadoop classpath):$HADOOP_HOME/share/hadoop/tools/lib/*" >> $SPARK_HOME/conf/spark-env.sh
RUN mv $HADOOP_HOME/share/hadoop/tools/lib/*.jar $HADOOP_HOME/share/hadoop/common/lib/
RUN rm /opt/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar
RUN find / -name jackson*2.9.8.* -exec rm -rf {} \;

WORKDIR /opt/spark/work-dir
COPY driver-pod-template.yaml /opt/spark/work-dir
COPY executor-pod-template.yaml /opt/spark/work-dir

ENV SPARK_DIST_CLASSPATH=/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*:/opt/hadoop/share/hadoop/tools/lib/*

ENTRYPOINT [ "/opt/spark/kubernetes/dockerfiles/spark/entrypoint.sh" ]
#ENTRYPOINT [ "/bin/bash" ]
